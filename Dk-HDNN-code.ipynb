{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"YucLRetUM4w2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604566820285,"user_tz":-540,"elapsed":141293,"user":{"displayName":"Niaz Ahmad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8B84KyLLq_bVPeViGwky9nd163Xr1g1ChUwFAJw=s64","userId":"04920252295002220425"}},"outputId":"51b6a3e2-8e4b-428d-f4b2-055236ff44ee"},"source":["# The code below will:\n","# Preprocess the text data (tokenization, stemming, POS tagging, and extracting sentiment words using a sentiment lexicon).\n","# Use RoBERTa to obtain word embeddings.\n","# Use BiGRU with an attention mechanism to select important features.\n","# Apply PCA for dimensionality reduction.\n","# Use a sigmoid activation function to classify the sentiment as positive or negative.\n","# Train the model with the training dataset, then evaluate and test the model on the test dataset.\n","import numpy as np\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import sentiwordnet as swn\n","from nltk.stem import PorterStemmer\n","from nltk import pos_tag\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from transformers import RobertaTokenizer, RobertaModel\n","import torch\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Bidirectional, GRU, Attention, Dense\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score\n","# Download necessary NLTK resources\n","nltk.download('punkt')\n","nltk.download('sentiwordnet')\n","nltk.download('averaged_perceptron_tagger')\n","# Initialize stemmer\n","stemmer = PorterStemmer()\n","# Define function to get sentiment score from Wide Coverage Sentiment Lexicon\n","def get_sentiment_score(word):\n","try:\n","synsets = list(wcsl.senti_synset(word+'.a.01')) # synset for sentiment analysis\n","sentiment_score = 0\n","for synset in synsets:\n","sentiment_score += synset.pos_score() - synset.neg_score()\n","return sentiment_score\n","except:\n","return 0\n","# Text preprocessing function\n","def preprocess_text(text):\n","tokens = word_tokenize(text.lower()) # Tokenize and convert to lowercase\n","pos_tags = pos_tag(tokens) # Part of speech tagging\n","stemmed_tokens = [stemmer.stem(token) for token, tag in pos_tags if tag in ['NN', 'VB', 'JJ', ‘RB’]] # Stemming and filtering based on POS\n","sentiment_score = sum([get_sentiment_score(word) for word in stemmed_tokens]) # Sentiment score based on lexicon\n","return stemmed_tokens, sentiment_score\n","# Sample data: Reviews and labels (1: Positive, 0: Negative)\n","reviews = [\n","\"I loved this movie! Great acting.\",\n","\"Terrible movie, very boring.\",\n","\"Amazing movie with stunning visuals!\",\n","\"Not worth the watch, it was a waste of time.\",\n","\"Fantastic movie, a true masterpiece!\",\n","\"The plot was boring and predictable.\",\n","\"Really good film, I would recommend it!\",\n","\"Poorly made, no substance at all.\",\n","\"Absolutely fantastic, will watch again!\",\n","\"It was okay, nothing special though.\"\n","]\n","labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 1] # Sentiment labels\n","# Preprocess all reviews\n","processed_data = [preprocess_text(review) for review in reviews]\n","# Tokenized and sentiment scores\n","tokens = [item[0] for item in processed_data]\n","sentiment_scores = [item[1] for item in processed_data]\n","# Apply RoBERTa for word embeddings\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","roberta_model = RobertaModel.from_pretrained('roberta-base')\n","def get_roberta_embeddings(text):\n","tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n","with torch.no_grad():\n","embeddings = roberta_model(**tokens).last_hidden_state\n","return embeddings.mean(dim=1).numpy() # Get average embedding for the sentence\n","# Get embeddings for all reviews\n","embeddings = [get_roberta_embeddings(review) for review in reviews]\n","embeddings = np.array(embeddings).reshape(len(embeddings), -1)\n","# Standardize the data before PCA\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(embeddings)\n","# Apply PCA to reduce dimensionality to 2 components\n","pca = PCA(n_components=2)\n","X_pca = pca.fit_transform(X_scaled)\n","# Reshape PCA result for input into GRU model\n","X_pca_reshaped = X_pca[..., np.newaxis]\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X_pca_reshaped, labels, test_size=0.2, random_state=42)\n","# Build the BiGRU model with attention mechanism\n","def build_bigru_attention_model(input_shape):\n","model = Sequential()\n","model.add(Bidirectional(GRU(64, return_sequences=True), input_shape=input_shape))\n","model.add(Attention())\n","model.add(Dense(1, activation='sigmoid')) # Binary output layer for sentiment classification\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","return model\n","# Initialize the model\n","model = build_bigru_attention_model((X_pca_reshaped[0].shape[0], 1))\n","model.summary()\n","# Train the model\n","history = model.fit(X_train, y_train, epochs=5, batch_size=2, validation_data=(X_test, y_test))\n","# Evaluate the model on the test set\n","y_pred = (model.predict(X_test) > 0.5).astype(int)\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f'Accuracy: {accuracy * 100:.2f}%')\n","# Visualize the PCA result\n","plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', marker='o')\n","plt.title('PCA of Movie Reviews')\n","plt.xlabel('Principal Component 1')\n","plt.ylabel('Principal Component 2')\n","plt.colorbar(label='Review Sentiment (0: Negative, 1: Positive)')\n","plt.show()\n","# Explanation of Code:\n","# Text Preprocessing: The preprocess_text function handles tokenization, part-of-speech tagging, stemming, and sentiment word extraction using the SentiWordNet lexicon.\n","# RoBERTa Word Embedding: We use the RoBERTa model to transform the reviews into high-dimensional embeddings.\n","# BiGRU Model with Attention: A BiGRU layer with an attention mechanism is used to capture important sequential features. The output layer uses a sigmoid function for binary classification (positive or negative).\n","# PCA for Dimensionality Reduction: PCA\n","# could you please give me the output in plot or chart form for the above code\n","# To generate the output as a plot or chart, we can use the PCA visualization to show the reduced-dimensionality results of the movie reviews. The plot will display the two principal components from the PCA, colored based on the sentiment labels (0 for Negative, 1 for Positive).\n","# Here’s how the code visualizes the PCA result, including a scatter plot with the sentiment labels:\n","# PCA Visualization: After applying PCA for dimensionality reduction to 2 components, we plot the results on a scatter plot. The points will be colored according to their sentiment (0: Negative, 1: Positive).\n","# Accuracy Plot: The model's training and validation accuracy can also be plotted to visualize how well the model performs over epochs.\n","# How to Generate the Plot\n","# The matplotlib library can be used for visualization. Below is the relevant part of the code that shows the PCA visualization. This code will plot the PCA results, and we will also plot training vs validation accuracy for the model.\n","import matplotlib.pyplot as plt\n","# PCA visualization: Plot the first two principal components\n","plt.figure(figsize=(8, 6))\n","scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', marker='o')\n","plt.title('PCA of Movie Reviews')\n","plt.xlabel('Principal Component 1')\n","plt.ylabel('Principal Component 2')\n","plt.colorbar(scatter, label='Review Sentiment (0: Negative, 1: Positive)')\n","plt.show()\n","# Plot the accuracy of the model during training\n","plt.figure(figsize=(8, 6))\n","plt.plot(history.history['accuracy'], label='Training Accuracy')\n","plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","plt.title('Model Accuracy during Training')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()\n","\n","Build and Train the Model (model.fit())\n","# Sample sentiment labels (binary: 1 for positive, 0 for negative)\n","y = [1, 0] # Example: Positive, Negative sentiment for the reviews\n","# Reshape PCA result for GRU model input\n","X_train_reshaped = X_pca[..., np.newaxis]\n","# Split the data into training and testing sets\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X_train_reshaped, y, test_size=0.2, random_state=42)\n","# Train the model\n","history = model.fit(X_train, y_train, epochs=5, batch_size=4, validation_data=(X_test, y_test))\n","# Evaluate the model\n","y_pred = (model.predict(X_test) > 0.5).astype(int)\n","from sklearn.metrics import accuracy_score\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f'Accuracy: {accuracy * 100:.2f}%')\n","Evaluate the Model (model.predict())\n","# To predict on new test data, use model.predict()\n","test_data = X_test # Your test data here\n","predictions = (model.predict(test_data) > 0.5).astype(int) # Predict class labels (0 or 1)\n","# You can compare predictions with true labels (y_test) for performance evaluation\n","print(f'Predictions: {predictions}')\n","print(f'True Labels: {y_test}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[<module>] => loading model from /content/drive/My Drive/ehpi_action_recognition/data/models/pose_resnet_50_256x192.pth\n","Traceback (most recent call last):\n","  File \"run_ehpi.py\", line 160, in <module>\n","    \"{}.avi\".format(str(frame_nr).zfill(5))), img)\n","TypeError: only size-1 arrays can be converted to Python scalars\n"],"name":"stdout"}]}]}